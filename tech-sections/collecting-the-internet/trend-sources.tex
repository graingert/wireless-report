\subsection{Design}
Trend Analysis Tasks should be implemented to take advantage of multiple different sources of trending topics to be used as keywords for URL Discovery tasks.

\paragraph{}
Sources that the system will use include the Twitter Trending Topics API, RSS Feeds and trends from derived from the Sun JSON Feed.

\subsubsection{Twitter}
The Twitter API is available in multiple versions, 1 and 1.1. Currently 1 is deprecated and it is recommended that new applications build against 1.1.  The disadvantage of the 1.1 API is that it requires a ``User Context'' so all requests must be performed as an authenticated user.This requires both a Twitter User and Application OAuth token.

\paragraph{}
As part of the Twitter API access to global trending topics is made available at a JSON endpoint\footnote{\url{https://api.twitter.com/1.1/trends/place.json}}. This is possible to parse manually but a library should be chosen to handle the interaction and authentication instead.

\paragraph{}
Topics with a pound sign ``\#'' as the first character are Twitter ``hashtags'' and are usually camel cased. But these hashtags must be converted from camel cased to space delimited so that the topic can be used as a set of keywords. For example ``\#BrazilWantsOneDirection'' becomes ``Brazil Wants One Direction'' and ``\#XFactorPHFinale'' becomes ``X Factor PH Final''.

\paragraph{}
In the case of all capitals eg ``NIALLSAYHITOTURKEY'' an entirely lowercase keyword should be derived.

\subsubsection{RSS}
A set of RSS feeds should be collected and parsed, ``feedparser'' is likely to be useful for this.  Each item from a feed should be entered into a corpus, to calculate keywords from items in the corpus algorithms such as TFIDF should be used. The natural language library ``metanl'' is likely to be helpful. Note that each feed can be processed separately with one corpus per feed.

\subsubsection{The Sun JSON API}
The Sun provides a JSON API for the top 200 most commented on articles\footnote{\url{http://bootstrap.thesun.fyre.co/api/cust/ni/todays_hottest/200.json}}. In most cases the content of this JSON file can be treated in exactly the same way as an RSS feed however care should be taken to deal with the strange encoding of some of the returned headlines: \verb`Terry a â\u0080\u0098liarâu\u0080\u0099` should be \verb`Terry a "liar"`.

\subsubsection{Alternative Sources}
Examples include:
\begin{itemize}
    \item Link aggregation sites such as Reddit and Digg.
    \item Search engine common searches and trends such as Google, Bing and Yahoo trends services.
    \item Network logs such as the campus search trends. It should be noted that this data is very unlikely to be accessible by malware distribution networks and might allow us to see trends before they do.
\end{itemize}

%implementation
\subsection{Implementation}
The most popular Twitter library for Python is called Tweepy, which is also 
our choice. This library provides access to Twitter version 1.1 API, which 
requires OAuth authentication. Therefore we created a Twitter account 
dedicated for this program, which provides consumer key and access token in 
order to pass the authentication. \\
The program then retrieves global trending keywords from the Twitter's {\em 
trends/place} API. As previous described, in order to cope with keywords with 
hashtags, we implemented a hashtag handler, who's role is to remove the hash 
tag and convert the camel case word to sentence case. This is achieved by 
applying regular expression substitution: 
\begin{verbatim}
re.sub(r'((?<=[a-z])[A-Z]|(?<!\A)[A-Z](?=[a-z]))',r' \1',
camelCaseString)
\end{verbatim}
Before the conversion happens the keyword is also decoded into unicode. The 
reason is that Celery does not support Python 2 strings with non-ASCII characters
which may lead to 
crash when Celery is sending them to RabbitMQ. 
\paragraph{}
A list of famous news websites' RSS feed addresses are hard coded in the 
program, the website list includes BBC, NewYork Times, Guardian, The Washington 
Post and so on. The RSS parser downloads every feeds, parses them into list 
then extracts news titles in them. Characters require encodings other than 
UTF-8 are ignored. The downloading is achieved with requests and any network 
errors will be handled in order to prevent system crash. 
\paragraph{}
The top 200 commented articles from Sun JSON API are handled in a similar way 
as that of RSS feeds. Due to good compatibility between Python and JSON the 
extraction of article titles were completed without any difficulties. 
\paragraph{}
Twitter trends, RSS feeds and Sun JSON are implemented as three different 
functions which are going to be run as separate Celery tasks. All of them 
return a list of keywords or sentences to the main system. 
